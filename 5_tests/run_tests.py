#!/usr/bin/env python3
"""
run_tests.py - Master script to run all test suites and generate a combined report

This script runs all the testing frameworks for the data cleaning pipeline:
1. General test framework for testing do-files and pipelines
2. Data integrity tests for testing data quality and consistency
3. Edge case tests for checking potential issues in the data cleaning process

Usage:
    python run_tests.py [--root /path/to/project/root]
    
    If --root is not provided, the script will auto-detect the root path
    based on the current username (same logic as in 1_master.do).

Author: Based on UGUR DIKTAS and JELKE CLARYSSE's data cleaning code
Date: March 2025
"""

import os
import sys
import argparse
import subprocess
import pandas as pd
import logging
from datetime import datetime
import time
import re
import glob
import getpass

def determine_root_path():
    """
    Determine the root path based on username similar to 1_master.do.
    Falls back to current directory if no match is found.
    """
    username = getpass.getuser()
    
    # Define roots for different users
    user_roots = {
        "jelkeclarysse": "/Users/jelkeclarysse/Library/CloudStorage/OneDrive-Universit채tZ체richUZH/3_STUDENTS/13_Cleaning",
        "ugurdiktas": "/Users/ugurdiktas/Library/CloudStorage/OneDrive-Universit채tZ체richUZH/3_STUDENTS/13_Cleaning"
    }
    
    # Return root path for current user or default to current directory
    if username in user_roots:
        root_path = user_roots[username]
        if os.path.exists(root_path):
            return root_path
            
    # Fallback to current directory
    return os.path.abspath(os.getcwd())

# Set up logging
def setup_logging(root_dir):
    """Configure logging to file and console."""
    log_dir = os.path.join(root_dir, "tests", "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"master_test_run_{timestamp}.log")
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(), timestamp, log_file

def run_python_test(script_path, root_dir):
    """Run a Python-based test script."""
    logger.info(f"Running Python test: {script_path}")
    try:
        result = subprocess.run(
            [sys.executable, script_path, "--root", root_dir],
            capture_output=True,
            text=True,
            check=False
        )
        
        if result.returncode != 0:
            logger.error(f"Python test failed with return code {result.returncode}")
            logger.error(f"Error output: {result.stderr}")
            return False, result.stderr
        else:
            logger.info(f"Python test completed successfully")
            return True, result.stdout
    except Exception as e:
        logger.error(f"Error running Python test: {str(e)}")
        return False, str(e)

def run_stata_test(script_path, root_dir):
    """Run a Stata-based test script."""
    logger.info(f"Running Stata test: {script_path}")
    
    # Create a temporary wrapper script that sets the root directory
    temp_script = os.path.join(root_dir, "temp_wrapper.do")
    with open(temp_script, "w") as f:
        f.write(f'global root "{root_dir}"\n')
        f.write(f'do "{script_path}"\n')
    
    try:
        # Determine Stata command based on OS
        stata_cmd = "stata-mp" if os.name != "nt" else "stata-mp"
        
        result = subprocess.run(
            [stata_cmd, "-b", "do", temp_script],
            capture_output=True,
            text=True,
            check=False
        )
        
        # Clean up
        if os.path.exists(temp_script):
            os.remove(temp_script)
        
        if result.returncode != 0:
            logger.error(f"Stata test failed with return code {result.returncode}")
            logger.error(f"Error output: {result.stderr}")
            return False, result.stderr
        else:
            logger.info(f"Stata test completed successfully")
            return True, result.stdout
    except Exception as e:
        logger.error(f"Error running Stata test: {str(e)}")
        # Clean up
        if os.path.exists(temp_script):
            os.remove(temp_script)
        return False, str(e)

def parse_test_results(root_dir):
    """Parse test results from all the Excel files generated by the tests."""
    logger.info("Parsing test results")
    
    # Find all Excel result files
    result_files = glob.glob(os.path.join(root_dir, "tests", "*.xlsx"))
    result_files += glob.glob(os.path.join(root_dir, "tests", "logs", "*.xlsx"))
    
    if not result_files:
        logger.warning("No test result files found")
        return None
    
    # Combine results from all files
    all_results = []
    
    for file in result_files:
        try:
            df = pd.read_excel(file)
            # Add source file information
            df['source_file'] = os.path.basename(file)
            all_results.append(df)
        except Exception as e:
            logger.error(f"Error parsing result file {file}: {str(e)}")
    
    if not all_results:
        logger.warning("No test results could be parsed")
        return None
    
    # Combine all results
    combined_results = pd.concat(all_results, ignore_index=True)
    return combined_results

def generate_report(results_df, root_dir, timestamp):
    """Generate a combined HTML report from the test results."""
    logger.info("Generating combined report")
    
    if results_df is None:
        logger.error("No results data available for report generation")
        return
    
    # Create reports directory
    reports_dir = os.path.join(root_dir, "tests", "reports")
    os.makedirs(reports_dir, exist_ok=True)
    
    # Generate summary statistics
    total_tests = len(results_df)
    pass_count = len(results_df[results_df['result'] == 'PASS'])
    fail_count = len(results_df[results_df['result'] == 'FAIL'])
    error_count = len(results_df[results_df['result'] == 'ERROR'])
    warning_count = len(results_df[results_df['result'].isin(['WARNING', 'PARTIAL'])])
    
    pass_rate = (pass_count / total_tests) * 100 if total_tests > 0 else 0
    
    # Generate HTML report
    html_report = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Data Cleaning Test Report - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            h1, h2 {{ color: #2c3e50; }}
            .summary {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}
            .pass {{ color: green; }}
            .fail {{ color: red; }}
            .warning {{ color: orange; }}
            .error {{ color: darkred; }}
            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            tr:nth-child(even) {{ background-color: #f9f9f9; }}
            .category-header {{ background-color: #e9ecef; font-weight: bold; }}
        </style>
    </head>
    <body>
        <h1>Data Cleaning Pipeline Test Report</h1>
        <p>Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        
        <div class="summary">
            <h2>Summary</h2>
            <p><strong>Total Tests:</strong> {total_tests}</p>
            <p><strong>Passed:</strong> <span class="pass">{pass_count} ({pass_rate:.1f}%)</span></p>
            <p><strong>Failed:</strong> <span class="fail">{fail_count}</span></p>
            <p><strong>Errors:</strong> <span class="error">{error_count}</span></p>
            <p><strong>Warnings:</strong> <span class="warning">{warning_count}</span></p>
        </div>
        
        <h2>Detailed Results</h2>
    """
    
    # Group results by test category for better organization
    grouped_results = results_df.groupby('test_category')
    
    html_report += """
        <table>
            <tr>
                <th>Test Category</th>
                <th>Test Name</th>
                <th>Result</th>
                <th>Notes</th>
            </tr>
    """
    
    for category, group in grouped_results:
        html_report += f"""
            <tr class="category-header">
                <td colspan="4">{category}</td>
            </tr>
        """
        
        for _, row in group.iterrows():
            result_class = {
                'PASS': 'pass',
                'FAIL': 'fail',
                'ERROR': 'error',
                'WARNING': 'warning',
                'PARTIAL': 'warning'
            }.get(row['result'], '')
            
            notes = row.get('notes', '')
            if pd.isna(notes):
                notes = ''
            
            html_report += f"""
                <tr>
                    <td>{category}</td>
                    <td>{row['test_name']}</td>
                    <td class="{result_class}">{row['result']}</td>
                    <td>{notes}</td>
                </tr>
            """
    
    html_report += """
        </table>
    </body>
    </html>
    """
    
    # Write HTML report to file
    report_file = os.path.join(reports_dir, f"test_report_{timestamp}.html")
    with open(report_file, 'w') as f:
        f.write(html_report)
    
    logger.info(f"Report generated: {report_file}")
    
    # Also save combined results to Excel
    excel_file = os.path.join(reports_dir, f"combined_results_{timestamp}.xlsx")
    results_df.to_excel(excel_file, index=False)
    logger.info(f"Combined results saved to Excel: {excel_file}")
    
    return report_file, excel_file

def run_all_tests(root_dir):
    """Run all test suites."""
    logger.info(f"Running all tests from project root: {root_dir}")
    
    # Define the test scripts to run
    test_scripts = [
        {"path": os.path.join(os.path.dirname(__file__), "test_framework.py"), "type": "python"},
        {"path": os.path.join(os.path.dirname(__file__), "data_integrity_tests.py"), "type": "python"},
        {"path": os.path.join(os.path.dirname(__file__), "stata_test_script.do"), "type": "stata"},
        {"path": os.path.join(os.path.dirname(__file__), "edge_case_tests.do"), "type": "stata"}
    ]
    
    # Run each test script
    results = {}
    
    for script in test_scripts:
        script_path = script["path"]
        script_type = script["type"]
        script_name = os.path.basename(script_path)
        
        # Check if script exists
        if not os.path.exists(script_path):
            logger.warning(f"Script not found: {script_path}")
            results[script_name] = {"success": False, "output": "Script not found"}
            continue
        
        logger.info(f"Running {script_type} script: {script_name}")
        start_time = time.time()
        
        if script_type == "python":
            success, output = run_python_test(script_path, root_dir)
        else:  # stata
            success, output = run_stata_test(script_path, root_dir)
        
        elapsed_time = time.time() - start_time
        logger.info(f"Completed in {elapsed_time:.2f} seconds with {'success' if success else 'failure'}")
        
        results[script_name] = {"success": success, "output": output, "elapsed_time": elapsed_time}
    
    # Parse and combine test results
    combined_results = parse_test_results(root_dir)
    
    # Generate report
    if combined_results is not None:
        report_file, excel_file = generate_report(combined_results, root_dir, timestamp)
        logger.info(f"Full test suite completed. Report at: {report_file}")
        logger.info(f"Combined results at: {excel_file}")
    else:
        logger.warning("Unable to generate report due to missing result data")
    
    # Return results
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run all test suites for data cleaning pipeline')
    parser.add_argument('--root', required=False, help='Path to project root directory')
    
    args = parser.parse_args()
    
    # Use provided root or auto-detect
    if args.root:
        root_dir = os.path.abspath(args.root)
    else:
        root_dir = determine_root_path()
        print(f"Auto-detected root path: {root_dir}")
    
    # Check if directory exists
    if not os.path.isdir(root_dir):
        print(f"Error: Directory not found: {root_dir}")
        sys.exit(1)
    
    # Set up logging
    logger, timestamp, log_file = setup_logging(root_dir)
    
    # Create tests directory structure if it doesn't exist
    tests_dir = os.path.join(root_dir, "tests")
    os.makedirs(tests_dir, exist_ok=True)
    
    # Start and time the test suite
    start_time = time.time()
    logger.info(f"Starting test suite run at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Run all tests
    results = run_all_tests(root_dir)
    
    # Calculate and log total time
    total_time = time.time() - start_time
    logger.info(f"Test suite completed in {total_time:.2f} seconds")
    
    # Print summary to console
    print("\n" + "="*80)
    print(f"TEST SUITE COMPLETE - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total runtime: {total_time:.2f} seconds")
    print(f"Log file: {log_file}")
    
    # Check each script result
    success_count = sum(1 for result in results.values() if result["success"])
    print(f"Scripts: {success_count}/{len(results)} completed successfully")
    
    for script_name, result in results.items():
        status = "SUCCESS" if result["success"] else "FAILURE"
        time_str = f"{result['elapsed_time']:.2f}s" if "elapsed_time" in result else "N/A"
        print(f"  - {script_name}: {status} ({time_str})")
    
    print("="*80)